自動運転E2Eモデル向け
データ多様性・希少性確保のためのデータフィルタリング戦略
1. 目的
本資料の目的は、自動運転E2Eモデルの初回学習データセット構築、および継続的なカバレッジ拡大のために、「データバリエーション（多様性）の最大化」と「希少データ（レアケース）の効率的な収集」に主眼を置いたデータフィルタリング手法を検討・評価することである。
2. 「高難易度フィルタリング」との違い
前回の「高難易度フィルタリング」（例：シャドウモード、急挙動の検出）は、**モデルがすでに知っているかもしれないが、苦手とするシーン（既知の難所）**を特定し、その精度を改善することに重点を置いていた。
対して「多様性・希少性フィルタリング」は、**モデルがまだ見たことのない、あるいは統計的に極めてまれなシーン（未知の状況、分布外データ）**を特定し、データセットのカバレッジ（網羅性）を広げることを最優先とする。
例えば、「珍しい彫刻が置いてあるだけの単純な直進道路」は高難易度ではないが、希少性は高いため、後者の観点では重要なデータとなる。
3. 多様性・希少性確保のための主要手法（妥当性順）
データバリエーション確保の観点から、妥当性が高いと考えられる順に手法を列挙する。
1. 特徴量空間に基づく多様性サンプリング
概要:
収集したデータ（主に画像）を、学習済みモデル（E2Eモデルの中間層や、自己教師あり学習モデルなど）を用いて低次元の特徴量ベクトル（Embeddings）に変換する。この特徴量空間上で、データポイントの分布を分析する。
Core-Set Selection（コアセット選択）:
データセット全体の分布を最もよく代表するような、多様性に富んだデータのサブセットを選択する手法。特徴量空間全体を効率よくカバーするデータ群を選ぶ。[4]
Outlier / Novelty Detection（外れ値・新規性検出）:
すでに学習データとして保有しているデータ群の分布から、特徴量空間上で距離が遠いデータ（＝新規性が高い、珍しいデータ）を優先的にサンプリングする。
妥当性評価:
多様性確保:
「データバリエーションの確保」という目的に対して、最も直接的かつ強力なアプローチである。データが「高難易度」かどうかに関わらず、純粋に「まだ持っていない種類」のデータを特定できる。
希少性確保:
外れ値検出は、定義上「希少なデータ」を特定する手法そのものである。
実行コスト:
全データの特徴量抽出と、大規模データ間での類似度計算やクラスタリングが必要であり、計算コストは（中〜高）となる。
2. モデルの不確実性（Uncertainty）に基づくサンプリング
概要:
学習済みモデルにデータを入力し、その予測の「不確実性（Uncertainty）」をスコアリングする。不確実性が高いデータとは、モデルが「予測に自信がない」データであり、これは学習データ分布から外れている（OOD: Out-of-Distribution）可能性が高いことを示す。[3]
（例：アンサンブルモデルの予測のばらつき、MC Dropout）
妥当性評価:
多様性確保:
モデルが知らない（＝学習データの分布にない）データを効率的に発見できるため、多様性向上に大きく寄与する。
希少性確保:
「不確実性が高い」データは、統計的に「希少な」データである可能性が非常に高い。
実行コスト:
オフラインでの推論処理が必要。（中）
3. シナリオ・メタデータに基づく層化サンプリング
概要:
軽量なアノテーション（例：物体検出で「歩行者」「自転車」を検出）や、CAN/Ego State/GPS/地図から取得できるメタデータ（例：天候、時間帯、道路種別）に基づき、データを複数の層（ストラタ）に分類する。
例えば、「雨天」「夜間」「交差点」「歩行者あり」といったタグの組み合わせでデータを分類し、統計的に希少な組み合わせ（例：雨天・夜間・高速道路合流）のデータを、意図的により多くサンプリングする（層化サンプリング）。
妥当性評価:
多様性確保:
「既知の多様性軸」（天候、時間帯など）におけるカバレッジを確実に担保できる。
希少性確保:
「珍しい組み合わせ」を定義し、その希少性を定量的に評価・サンプリングできる。
実行コスト:
メタデータの取得・処理のみで実行できるため、コストは（低）である。ただし、未知の希少性（例：道路上の予期せぬ落下物）は検出できない。
（参考）高難易度シグナル（ディスエンゲージメント、急挙動）
概要:
手動介入（ディスエンゲージメント）や、急ブレーキ・急ハンドルといったドライバーの危険回避行動。
妥当性評価:
これらは「高難易度」であると同時に、全走行データの中では統計的に「希少」なイベントでもある。したがって、これらを収集することは、多様性の中でも特に重要な「危険な希少シーン」のカバレッジを確保する上で有効である。
ただし、これだけでは多様性全体（例：危険ではないが珍しい風景、珍しい道路構造）をカバーすることはできないため、上記1〜3の手法と組み合わせることが不可欠である。
4. 結論と推奨アプローチ
初回学習データセットの多様性を最大化し、希少データを効率的に確保するためには、以下の段階的なアプローチを推奨する。
ベースライン（コスト低）:
まず「3. シナリオ・メタデータに基づく層化サンプリング」を導入する。天候、時間帯、道路種別（交差点、合流、カーブ）といった基本的なメタデータでデータを分類し、各カテゴリが一定数含まれるように（特に希少なカテゴリを厚めに）サンプリングする。
多様性の最大化（コスト中〜高）:
次に「1. 特徴量空間に基づく多様性サンプリング」を導入する。全データを特徴量空間にマッピングし、既存のデータセットから最も遠い（新規性が高い）データを優先的に追加していく。これにより、メタデータでは分類できない「未知の多様性」を確保する。
弱点補強（コスト中）:
モデルの学習がある程度進んだ段階で「2. モデルの不確実性サンプリング」を併用する。これにより、現在のモデルが特に理解できていない（＝分布外の）データを特定し、効率的にカバレッジを拡大する。
5. 参考文献
[1] Tesla AI Day 2021 & 2022 (特に "Data Engine" に関するセクション)
[2] Waymo Safety Report (ディスエンゲージメントに関する記述)
[3] Gal, Y., & Ghahramani, Z. (2016). "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning." ICML.
[4] Philion, J., et al. (2023). "Scaling End-to-End Autonomous Driving with Data-Driven Continuous Learning." CVPR 2023. (Waymoによるデータ駆動型アプローチ。特徴量空間でのデータセット分析に言及)
[5] Sener, O., & Savarese, S. (2018). "Active Learning for Convolutional Neural Networks: A Core-Set Approach." ICLR. (Core-Set選択の代表的な研究)


自動運転E2Eモデル向け データフィルタリング手法の比較検討
1. 目的
自動運転E2Eモデルの学習において、大量の収集データから学習に寄与しにくいデータ（例：単純な直進）を削減し、モデルの性能向上に直結する高難易度シーンや希少なデータを効率的に抽出（フィルタリング）する手法を整理・評価する。
2. 評価軸
各手法を以下の3つの観点で評価する。
実行コスト:
低：CAN/Ego State等のメタデータのみで処理可能。
中：軽量なモデル推論やオフラインでの計算が必要。
高：全データへの重いモデル推論（例：物体検出・追跡）や人手によるアノテーションが必要。
フィルタリング精度（高難易度シーンの特定）:
高：モデルが苦手とするシーンや、危険シーンを直接的に特定できる。
中：高難易度シーンと相関があるが、ノイズ（簡単だが条件に合致するシーン）も含む。
低：間接的な指標であり、高難易度とは限らない。
検索再利用性:
高：付与されたメタデータ（タグ、スコア、特徴量）が、後段のデータ検索や分析に非常に有用。
中：検索キーとして利用可能だが、限定的。
低：フィルタリング（採用/不採用）の1回きりの利用がメイン。
3. データフィルタリング手法（有効性順）
1. シャドウモード（実運用モデルの予測誤差）
概要:
テスト車両や（許可を得た）量産車に学習済みモデルを搭載し、リアルタイムで推論を実行する（ただし運転には介入しない）。モデルの予測（例：操舵角、加速度）と、実際のドライバーの操作を比較し、その乖離（予測誤差）が大きいデータを「モデルが苦手なデータ」としてアップロード・収集する手法。Teslaが「Data Engine」の中核として多用していることで知られる [1]。
評価:
実行コスト: 中〜高
実車へのモデルデプロイとリアルタイム推論環境が必要。データアップロードの通信コストも発生するが、全データを上げるよりは遥かに効率的。
フィルタリング精度: 高
「現在のモデルが実際に間違える（または迷う）シーン」を直接特定できるため、E2Eモデルの学習において最も効率的なデータと言える。
検索再利用性: 高
「予測誤差の大きさ」「どの予測値（操舵/加減速）で誤差が大きかったか」をメタデータとして保存でき、検索・分析に極めて有用。
2. ディスエンゲージメント（手動介入）データ
概要:
自動運転のテスト走行中に、安全性や快適性の問題からシステムが自動運転を解除した（あるいはセーフティドライバーが手動で介入した）シーンのデータを収集する。これは「現在のシステムでは対応できない」ことが明確になった高難易度シーンの直接的なシグナルである。
評価:
実行コスト: 低
手動介入のログ（時刻、位置、理由）は通常、テスト走行で必ず取得されるため、追加コストはほぼゼロ。
フィルタリング精度: 高
「モデルの失敗」という最も重要なデータセット。Waymoなどの多くの企業が、このディスエンゲージメント・レートをKPIとしている [2]。
検索再利用性: 高
「介入発生」「介入理由（例：他車割り込み、認識ミス）」といったタグは、検索において非常に強力なキーとなる。
3. 自車（Ego Vehicle）の急挙動
概要:
CANバスやEgo Stateから取得できる自車の状態に基づき、急ブレーキ（高い負の加速度）、急ハンドル（高い操舵角速度）、急加速などを検知する。これらは、ドライバーが危険回避行動を取った可能性が高いシーンを示す。
評価:
実行コスト: 低
CANデータ（加速度、ヨーレート、操舵角）の時系列データに対して、単純な閾値処理で抽出可能。
フィルタリング精度: 中〜高
高難易度シーンと強い相関がある。ただし、ドライバーの運転癖による不要な急操作もノイズとして含まれる可能性がある。
検索再利用性: 高
「急ブレーキ発生」「急ハンドル発生」およびその最大値（G、deg/s）をメタデータとして付与でき、検索に有用。
4. モデルの不確実性（Uncertainty）に基づくサンプリング
概要:
オフラインでデータ（主に画像）をモデルに入力し、その予測の「不確実性」をスコアリングする。不確実性が高いデータ（モデルが自信を持って予測できていないデータ）を優先的に選択する。不確実性の推定には、アンサンブルモデルの予測のばらつきや、Dropoutを用いたモンテカルロ法（MC Dropout）などが用いられる [3]。
評価:
実行コスト: 中
オフラインでの推論処理が必要。アンサンブルや複数回の推論（MC Dropout）を行う場合、通常の推論より計算コストがかかる。
フィルタリング精度: 高
モデルが「知らない」「迷っている」データを特定できるため、シャドウモードに近い効果がオフラインで得られる。
検索再利用性: 高
「不確実性スコア」をメタデータとして保存し、スコアの高い順に検索・サンプリングできる。
5. 特徴量空間での新規性・多様性サンプリング
概要:
学習済みモデル（E2Eモデルの中間層や、画像認識モデル）を用いて、入力データ（画像など）を低次元の特徴量ベクトル（Embeddings）に変換する。既に学習データセットに存在するデータ群と特徴量空間上で距離が遠いデータ（新規性が高いデータ、Outlier）や、データセット全体の多様性を最大化するようにデータをサンプリングする手法（例：Core-set selection）[4]。
評価:
実行コスト: 中〜高
全データの特徴量抽出と、大規模データ間での類似度計算（またはクラスタリング）が必要。
フィルタリング精度: 中
「まだ学習していない珍しいシーン」を抽出できるため、カバレッジ向上に有効。ただし、「珍しい」が必ずしも「高難易度」とは限らない（例：珍しいオブジェが映っているだけの簡単な直進）。
検索再利用性: 高
特徴量ベクトル自体をデータベースに保存し、類似画像検索（例：特定のシーンに似たシーンを探す）に活用できる。
6. 軽量アノテーション・計算によるシナリオ抽出
概要:
ユーザーが求めている「高難易度シーン」を定義し、それを検出するための軽量なアノテーションや計算処理を全データに適用する。
例1（メタデータ利用）: GPSと地図情報から「交差点」「合流地点」「分岐地点」「急カーブ（曲率半径が小さい）」を抽出。
例2（軽量モデル利用）: 軽量な物体検出モデルを動かし、「周辺車両のTTC（Time-to-Collision）が閾値以下」「他車の急な割り込み（横方向の移動量が大きい）」を検出。
評価:
実行コスト: 手法による（メタデータ利用なら低、軽量モデル利用なら中〜高）
全データに対して特定の検出ロジックを実行するコストがかかる。
フィルタリング精度: 中
「直進」を削減し、「交差点」や「カーブ」を増やすという点では有効。ただし、簡単な交差点やカーブも大量に含まれるため、それ自体が高難易度とは限らない。TTC計算などはより精度が高い。
検索再利用性: 高
「交差点」「TTC < 3.0s」「割り込み検知」など、具体的なシナリオタグを付与でき、検索性が非常に高い。
4. 結論と推奨アプローチ
E2Eモデルの学習においては、まず**実行コストが低く効果の高い手法（2. ディスエンゲージメント、3. 急挙動）**をベースラインとして導入し、単純な直進データや簡単な走行データを削減することを推奨します。
その上で、モデルの弱点を直接的に潰していくために、1. シャドウモード または 4. 不確実性サンプリング を導入することが、学習効率の最大化に繋がります。
さらに、データセットの網羅性（カバレッジ）を担保し、未知の状況に対応するために 5. 特徴量空間でのサンプリング を、特定のシナリオ（例：雨天、夜間、交差点）を強化するために 6. 軽量アノテーション を組み合わせることが理想的です。
5. 参考文献
[1] Tesla AI Day 2021 & 2022 (特に "Data Engine" に関するセクション)
[2] Waymo Safety Report (ディスエンゲージメントに関する記述)
[3] Gal, Y., & Ghahramani, Z. (2016). "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning." ICML. (MC Dropoutの原典)
[4] Philion, J., et al. (2023). "Scaling End-to-End Autonomous Driving with Data-Driven Continuous Learning." CVPR 2023. (Waymoによるデータ駆動型アプローチ。特徴量空間でのデータセット分析に言及)


# シナリオ・メタデータに基づく層化サンプリング」について、nuScenes のデータセットを具体的な例として、詳細な検討を行います。

### 1. nuScenesから抽出可能なサンプリング用データ項目

nuScenes のデータフォーマットおよび CAN-bus データから、層化サンプリングの「軸」として利用可能なデータ項目を以下のように抽出・分類します。

#### A. 環境・静的条件 (Environment & Static Context)

* **天候・時間帯:**
    * `scene.description`: シーンの説明文から "Rain" (雨), "Night" (夜), "Day" (昼), "Snow" (雪) などのキーワードを抽出。
* **地理・場所:**
    * `log.location`: "boston-seaport", "singapore-onenorth" など、都市や地域（例：シンガポール＝左側通行、ボストン＝右側通行）。
* **道路種別 (Mapデータ連携):**
    * `map` API を利用し、自車の位置と地図情報を照合。
    * `road_segment` (一般道), `intersection` (交差点), `lane_connector` (合流/分岐), `stop_line` (停止線付近) などを特定。

#### B. 自車挙動 (Ego Vehicle Behavior) (CAN-bus データ)

* **速度:**
    * `vehicle_monitor.vehicle_speed` (km/h) または `zoe_veh_info.veh_speed` (m/s): [停止, 低速, 中速, 高速] のようにビン化。
* **加減速:**
    * `imu.linear_accel`: 加速度の大きさ（特に急加速・急減速）。
* **操舵:**
    * `steer.steering`: ステアリング舵角（直進、緩カーブ、急カーブ）。
    * `steer.steering_speed`, `vehicle_monitor.yaw_rate`: 操舵速度やヨーレート（急ハンドル、レーンチェンジ）。

#### C. 動的交通状況 (Dynamic Traffic Situation)

* **オブジェクト密度 (Annotationデータ):**
    * `sample_annotation` を集計し、各フレームのオブジェクト数をカウント。
    * `human.pedestrian`: 歩行者数 [0, 1-2, 3+]。
    * `vehicle.car`: 他車数 [0-1, 2-5, 6+]。
    * `vehicle.bicycle` / `vehicle.motorcycle`: 二輪車数 [0, 1+]。
* **オブジェクトの状態 (Annotationデータ):**
    * `sample_annotation.attribute_tokens` を集計。
    * `vehicle.moving` (移動中), `vehicle.parked` (駐車中), `pedestrian.walking` (歩行中) などの割合。

---

### 2. 層化サンプリングのアルゴリズム

これらのデータ項目を利用した、具体的な層化サンプリングのアルゴリズムを以下に示します。

#### ステップ1: シナリオベクトルの生成 (Scenario Vectorization)

まず、収集した各データ（例：nuScenesの1シーンや、数秒間のクリップ）を、カテゴリ化された「シナリオベクトル」に変換します。

1.  **クリップ化:** 連続したデータを扱いやすい単位（例：5秒間のクリップ）に分割します。
2.  **メタデータ集約:** 各クリップについて、上記「1.」で抽出したデータ項目を集約します。（例：5秒間の平均車速、最大ヨーレート、出現したオブジェクトカテゴリ、天候など）
3.  **離散化 (ビニング):** 各データ項目を離散的なカテゴリ（ビン）に分類します。

    * `avg_speed` (平均車速): [停止 (0-1), 低速 (1-30), 中速 (30-60), 高速 (60+)] (km/h)
    * `max_yaw_rate` (最大ヨーレート): [直進 (小), カーブ (中), 急旋回 (大)]
    * `weather` (天候): [晴/昼, 曇/昼, 雨/昼, 晴/夜, 雨/夜]
    * `road_type` (道路種別): [一般道, 交差点, 合流/分岐]
    * `pedestrian_count` (歩行者数): [0, 1-2, 3+]

4.  **ベクトル生成:** 各クリップは、これらのビンの組み合わせからなる「シナリオベクトル」で表現されます。
    * 例：クリップA = `[雨/夜, 交差点, 低速, 3+]`

#### ステップ2: データ分布の統計分析 (Histogramming)

全データセット（例：100万クリップ）についてシナリオベクトルを生成し、その多次元ヒストグラム（各シナリオの組み合わせが何件存在するか）を作成します。

* `[晴/昼, 一般道, 中速, 0]` (晴天の直進): 500,000 クリップ (50%)
* `[雨/夜, 交差点, 低速, 3+]` (雨の夜の交差点、歩行者多数): 50 クリップ (0.005%)

#### ステップ3: サンプリング重みの決定 (Weighting)

各シナリオ（ビン）に対して、その希少性に基づいたサンプリング重み $W_{bin}$ を決定します。目的は、希少なデータの重みを上げ、豊富なデータの重みを下げることです。

* **目標分布に基づく重み付け (推奨):**
    データセットの目標分布（例：全てのシナリオで均等に1000件ずつ）を設定します。
    * $N_{bin}$: あるビンの現在のデータ数
    * $N_{target}$: そのビンの目標データ数（例: 1000）
    * **サンプリング確率 $P_{bin} = \min(1.0, N_{target} / N_{bin})$**

    * **例 ( $N_{target} = 1000$ の場合):**
        * **晴天の直進** ($N_{bin} = 500,000$):
            $P = 1000 / 500000 = 0.002$ (0.2%の確率でサンプリング)
        * **雨の夜の交差点** ($N_{bin} = 50$):
            $P = 1000 / 50 = 20 \rightarrow \min(1.0, 20) = 1.0$ (100%の確率でサンプリング)

#### ステップ4: サンプリングの実行 (Sampling)

1.  全データセットの各クリップ $D_i$ をスキャンします。
2.  $D_i$ が属するシナリオベクトル（ビン） $B_j$ を特定します。
3.  そのビンのサンプリング確率 $P_{B_j}$ に基づいて、確率的にサンプリングします。（例：$P=0.002$ なら、0から1の一様乱数を生成し、0.002未満なら採用）
4.  採用されたクリップ群が、多様性を担保した学習用データセットとなります。

---

### 3. フィルタリング構築に必要なタスク一覧

この層化サンプリング・フィルタリングシステムを構築するためのタスク一覧です。

1.  **要件定義・設計タスク:**
    * 多様性として担保したい「軸」（シナリオ）を定義・確定する。（上記1.の項目選定）
    * 各軸の「ビン」の切り方（閾値）を決定する。
    * 最終的な学習データセットの目標サイズと、各シナリオの目標分布（例：一様分布、あるいは特定の希少シナリオを重視）を決定する。

2.  **データエンジニアリング・パイプラインタスク:**
    * nuScenesの各種データ（JSON, pkl, CAN-bus）をパースし、時系列に沿ってクリップ単位（例：5秒）に分割する処理を実装する。
    * Map APIと連携し、各クリップに道路種別（交差点、合流など）のタグを付与する処理を実装する。
    * （ステップ1）クリップをシナリオベクトルに変換するデータパイプラインを構築する。

3.  **分析・実装タスク:**
    * （ステップ2）全データセットのシナリオ分布（多次元ヒストグラム）を集計・可視化する分析スクリプトを実装する。
    * （ステップ3）決定したアルゴリズムに基づき、各シナリオのサンプリング重み（確率）を計算するロジックを実装する。
    * （ステップ4）計算された確率に基づき、データセット全体からクリップをサンプリング（フィルタリング）する処理を実装する。

4.  **評価・運用タスク:**
    * フィルタリング（サンプリング）後のデータセットのシナリオ分布を再度可視化し、目標分布と比較して評価する。
    * （可能であれば）フィルタリング前後のデータセットでE2Eモデルを学習させ、性能（特に希少シナリオでの性能）を比較評価する。
    * 新しいデータが追加された際に、このパイプラインを再実行し、データセットを継続的に更新（キュレーション）する運用フローを整備する。